{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQkNB6gQ-wav"
      },
      "outputs": [],
      "source": [
        "# utilising musicgen to create music transitions between 2 songs\n",
        "# musicgen huggingface colab was helpful in creating this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xonZUD9C_gz7"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PZZq5f7_kTY"
      },
      "outputs": [],
      "source": [
        "# the installations that work for us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLVM293d_gqZ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet pip\n",
        "\n",
        "!pip uninstall -y cudf-cu12 pylibcudf-cu12\n",
        "!pip install --upgrade pyarrow>=21.0.0 datasets[audio]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9euD3Y9_gX7"
      },
      "outputs": [],
      "source": [
        "# loading in the model from musicgen\n",
        "\n",
        "from transformers import MusicgenForConditionalGeneration\n",
        "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3wUKh9f_7XJ"
      },
      "outputs": [],
      "source": [
        "# commands for ffmpeg to get the combined audio that is sent into the model:\n",
        "\n",
        "#   ffmpeg -sseof -5 -i audio1.mp3 -t 5 part1.mp3\n",
        "#   ffmpeg -ss 0 -i audio2.mp3 -t 5 part2.mp3\n",
        "#   echo -e \"file 'part1.mp3'\\nfile 'part2.mp3'\" > list.txt\n",
        "#   ffmpeg -f concat -safe 0 -i list.txt -c copy output.mp3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4nAHHjW_rYj"
      },
      "outputs": [],
      "source": [
        "# formatting the combined audio\n",
        "import torchaudio\n",
        "import torch\n",
        "wav1, sr1 = torchaudio.load(\"input.mp3\")\n",
        "resampler = torchaudio.transforms.Resample(orig_freq=sr1, new_freq=32000)\n",
        "wav1 = resampler(wav1.mean(dim=0, keepdim=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDx_xZhB_rPE"
      },
      "outputs": [],
      "source": [
        "# actual generation\n",
        "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
        "from IPython.display import Audio\n",
        "import torch\n",
        "import scipy\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
        "model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n",
        "\n",
        "inputs = processor(\n",
        "    audio=wav1.squeeze(),\n",
        "    sampling_rate=32000,\n",
        "    text=[\"there are two songs within this audio, blend them together and make a seamless transition\"],\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n",
        "Audio(audio_values[0].cpu().numpy(), rate=model.config.audio_encoder.sampling_rate)\n",
        "\n",
        "scipy.io.wavfile.write(\"musicgen_outcombined.wav\", rate=32000, data=audio_values[0, 0].cpu().numpy())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
