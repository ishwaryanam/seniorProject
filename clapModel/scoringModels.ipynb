{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d6eb69b-1ec8-4e37-ba58-eb36fd6ab677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.11/site-packages (4.26.1)\n",
      "Requirement already satisfied: librosa in /opt/anaconda3/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in /opt/anaconda3/lib/python3.11/site-packages (0.13.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (0.62.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.16.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /opt/anaconda3/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.45.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio transformers librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83cc94c-46f7-4ba8-9160-3d01c7b68cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in /opt/anaconda3/lib/python3.11/site-packages (0.25.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ce3516-1d00-4ffd-87fa-4990cd910ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92abc15-d9e1-442d-8c96-2e052940daa5",
   "metadata": {},
   "source": [
    "<h1><b>CLAP Model</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69672ba-3af8-4b7b-822e-166a05774ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qt/r5m0294d74l48zwf6_cc54wc0000gn/T/ipykernel_20114/3820826989.py:34: FutureWarning: `audios` is deprecated and will be removed in version v4.59.0 for `ClapProcessor.__call__`. Use `audio` instead.\n",
      "  inputs = processor(audios=[a_window, b_window], return_tensors=\"pt\", sampling_rate=sr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.7260153889656067\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import librosa, soundfile as sf\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load MP3 and export to WAV\n",
    "sound = AudioSegment.from_mp3(\"backToFriends.mp3\")\n",
    "sound.export(\"backToFriends.wav\", format=\"wav\")\n",
    "\n",
    "y, sr = librosa.load(\"backToFriends.wav\", sr = 48000)\n",
    "\n",
    "a = y[:30*sr]\n",
    "sf.write(\"songA.wav\", a, sr)\n",
    "\n",
    "# Take next 30s as Song B\n",
    "b = y[30*sr:60*sr]\n",
    "sf.write(\"songB.wav\", b, sr)\n",
    "\n",
    "a, sr = librosa.load(\"songA.wav\", sr=48000, mono = True)\n",
    "b, sr = librosa.load(\"songB.wav\", sr = 48000, mono=True)\n",
    "\n",
    "# Load CLAP model + processor\n",
    "model_id = \"laion/clap-htsat-fused\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# Slice last 2s of A and first 2s of B\n",
    "a_window = a[-5*sr:]\n",
    "b_window = b[:5*sr]\n",
    "\n",
    "# Process into model inputs\n",
    "inputs = processor(audios=[a_window, b_window], return_tensors=\"pt\", sampling_rate=sr)\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model.get_audio_features(**inputs)\n",
    "\n",
    "# Compare similarity\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1], dim=0)\n",
    "print(\"Similarity score:\", cosine_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4807e8-1a7c-45a6-88f8-657eff265a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'from_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Load CLAP model + processor\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaion/clap-htsat-fused\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Slice last 2s of A and first 2s of B\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/processing_auto.py:264\u001b[0m, in \u001b[0;36mAutoProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         processor_class \u001b[38;5;241m=\u001b[39m processor_class_from_name(processor_class)\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processor_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    265\u001b[0m         pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Last try: we use the PROCESSOR_MAPPING.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m PROCESSOR_MAPPING:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'from_pretrained'"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import librosa, soundfile as sf\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load MP3 and export to WAV\n",
    "sound = AudioSegment.from_mp3(\"backToFriends.mp3\")\n",
    "sound.export(\"backToFriends.wav\", format=\"wav\")\n",
    "\n",
    "sound2 = AudioSegment.from_mp3(\"finesse.mp3\")\n",
    "sound2.export(\"finesse.wav\", format=\"wav\")\n",
    "\n",
    "y, sr = librosa.load(\"backToFriends.wav\", sr = 48000)\n",
    "fi, srf = librosa.load(\"finesse.wav\", sr = 48000)\n",
    "\n",
    "a = y[:30*sr]\n",
    "sf.write(\"songA.wav\", a, sr)\n",
    "\n",
    "fines = fi[:30*srf]\n",
    "sf.write(\"fines.wav\", fines, srf)\n",
    "\n",
    "a, sr = librosa.load(\"songA.wav\", sr=48000, mono = True)\n",
    "fines, srf = librosa.load(\"fines.wav\", sr = 48000, mono = True)\n",
    "\n",
    "# Load CLAP model + processor\n",
    "model_id = \"laion/clap-htsat-fused\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# Slice last 2s of A and first 2s of B\n",
    "a_window = a[-5*sr:]\n",
    "f_window = fines[:5*sr]\n",
    "\n",
    "# Process into model inputs\n",
    "inputs = processor(audios=[a_window, f_window], return_tensors=\"pt\", sampling_rate=sr)\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model.get_audio_features(**inputs)\n",
    "\n",
    "# Compare similarity\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1], dim=0)\n",
    "print(\"Similarity score:\", cosine_sim.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2a351-37a4-49fa-9d43-adc965819117",
   "metadata": {},
   "source": [
    "<h1><b>MERT Model</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05cfc6f-e5b8-4a8a-9709-1b32edaaea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.9401344060897827\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch, librosa\n",
    "\n",
    "model_id = \"m-a-p/MERT-v1-330M\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Load audio at 24kHz\n",
    "a, sr = librosa.load(\"songA.wav\", sr=24000, mono=True)\n",
    "b, sr = librosa.load(\"songB.wav\", sr=24000, mono=True)\n",
    "\n",
    "# Take windows\n",
    "a_window = a[-3*sr:]\n",
    "b_window = b[:3*sr]\n",
    "\n",
    "# Put waveforms into a list\n",
    "waveforms = [a_window, b_window]\n",
    "\n",
    "# Preprocess (MERT expects raw_speech)\n",
    "inputs = processor(\n",
    "    raw_speech=waveforms,\n",
    "    sampling_rate=sr,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # mean pooling\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1], dim=0)\n",
    "print(\"Similarity score:\", cosine_sim.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ff0e43-8dda-4937-b19b-4b6fbbe10496",
   "metadata": {},
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch, librosa\n",
    "\n",
    "model_id = \"m-a-p/MERT-v1-330M\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Load audio at 24kHz\n",
    "a, sr = librosa.load(\"songA.wav\", sr=24000, mono=True)\n",
    "b, sr = librosa.load(\"fines.wav\", sr=24000, mono=True)\n",
    "\n",
    "# Take windows\n",
    "a_window = a[-2*sr:]\n",
    "b_window = b[:2*sr]\n",
    "\n",
    "# Put waveforms into a list\n",
    "waveforms = [a_window, b_window]\n",
    "\n",
    "# Preprocess (MERT expects raw_speech)\n",
    "inputs = processor(\n",
    "    raw_speech=waveforms,\n",
    "    sampling_rate=sr,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # mean pooling\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1], dim=0)\n",
    "print(\"Similarity score:\", cosine_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc0e69-e4d3-46e5-91dd-b39bcb726591",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Trying diff window lengths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e47af7b-c535-4a20-8151-81946ef2c71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window: 1s → Similarity: 0.6942\n",
      "Window: 2s → Similarity: 0.7546\n",
      "Window: 3s → Similarity: 0.7626\n",
      "Window: 4s → Similarity: 0.7685\n",
      "Window: 5s → Similarity: 0.7706\n",
      "\n",
      "Best window length: 5s (Similarity: 0.7706)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch, librosa\n",
    "import numpy as np\n",
    "\n",
    "model_id = \"m-a-p/MERT-v1-330M\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Load audio at 24kHz\n",
    "a, sr = librosa.load(\"songA.wav\", sr=24000, mono=True)\n",
    "b, sr = librosa.load(\"fines.wav\", sr=24000, mono=True)\n",
    "\n",
    "# Define window lengths (in seconds)\n",
    "window_lengths = [1, 2, 3, 4, 5]  # try 1–5 seconds\n",
    "best_score = -1\n",
    "best_window = None\n",
    "\n",
    "for w in window_lengths:\n",
    "    num_samples = int(w * sr) #have to do seconds times sampling rate (samples per second)\n",
    "\n",
    "    # Last w seconds of song A, first w seconds of song B\n",
    "    if len(a) < num_samples or len(b) < num_samples:\n",
    "        continue  # skip if song is too short\n",
    "\n",
    "    a_window = a[-num_samples:]\n",
    "    b_window = b[:num_samples]\n",
    "\n",
    "    # Preprocess\n",
    "    inputs = processor(\n",
    "        raw_speech=[a_window, b_window],\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(embeddings[0], embeddings[1], dim=0)\n",
    "    score = cosine_sim.item()\n",
    "\n",
    "    print(f\"Window: {w}s → Similarity: {score:.4f}\")\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_window = w\n",
    "\n",
    "print(f\"\\nBest window length: {best_window}s (Similarity: {best_score:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
